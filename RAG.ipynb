{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import library\n",
    "import os\n",
    "from langchain_community.document_loaders import PyPDFLoader\n",
    "\n",
    "openai_api_key = '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'\n",
    "directory = './documents/Deep Learning'\n",
    "# extract document from user query\n",
    "\n",
    "files = os.listdir(directory)\n",
    "\n",
    "# Filter out only PDF files\n",
    "pdf_files = [os.path.join(directory, file) for file in files if file.endswith('.pdf')]\n",
    "\n",
    "# Iterate over each PDF file and load it\n",
    "for pdf_file in pdf_files:\n",
    "    loader = PyPDFLoader(pdf_file)\n",
    "    data = loader.load()\n",
    "    \n",
    "data = loader.load()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "# extract topic from user query\n",
    "topic = \"Machine learning\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "example_json = [\n",
    "    {\n",
    "        \"id\": \"1\",\n",
    "        \"title\": \"\",\n",
    "        \"children\": [\n",
    "            {\n",
    "                \"id\": \"1.1\",\n",
    "                \"title\": \"\",\n",
    "                \"description\": \"\",\n",
    "                \"links\": [\"\"]\n",
    "            },\n",
    "            {\n",
    "                \"id\": \"1.2\",\n",
    "                \"title\": \"\",\n",
    "                \"description\": \"\",\n",
    "                \"links\": [\"\"],\n",
    "                \"children\": [\n",
    "                    {\n",
    "                        \"id\": \"1.2.1\",\n",
    "                        \"title\": \"\",\n",
    "                        \"description\": \"\",\n",
    "                        \"links\": [\"\"]\n",
    "                    },\n",
    "                ]\n",
    "            },\n",
    "            {\n",
    "                \"id\": \"1.3\",\n",
    "                \"title\": \"\",\n",
    "                \"description\": \"\",\n",
    "                \"links\": [\"\"]\n",
    "            }\n",
    "        ]\n",
    "    },\n",
    "    {\n",
    "        \"id\": \"2\",\n",
    "        \"title\": \"\",\n",
    "        \"children\": [\n",
    "            {\n",
    "                \"id\": \"\",\n",
    "                \"title\": \"\",\n",
    "                \"description\": \"\",\n",
    "                \"links\": [\"\", \"\"]\n",
    "            },\n",
    "        ]\n",
    "    }\n",
    "]\n",
    "\n",
    "example_query = json.dumps(example_json, indent=2)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_openai import OpenAIEmbeddings\n",
    "from langchain_text_splitters import RecursiveCharacterTextSplitter\n",
    "from langchain_community.vectorstores import Chroma\n",
    "\n",
    "# quote= \"Artem Bisliouk is a hockey player from Mannheim. He is very good at data science and 22 years old.\"\n",
    "\n",
    "splitter = RecursiveCharacterTextSplitter(\n",
    "    chunk_size=200,\n",
    "    chunk_overlap=50,\n",
    "    separators=['.'])\n",
    "\n",
    "docs = splitter.split_documents(data) \n",
    "# docs = splitter.split_text(quote) \n",
    "\n",
    "# Embed the documents and store them in a Chroma DB\n",
    "embedding_model = OpenAIEmbeddings(openai_api_key=openai_api_key)\n",
    "docstorage = Chroma.from_documents(docs, embedding_model)\n",
    "# docstorage = Chroma.from_texts(docs, embedding_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Document(page_content='Deep Learning\\n03 – Gradient-Based Training\\nPart 1: Backpropagation\\nProf. Dr. Rainer Gemulla\\nUniversität Mannheim\\nVersion: 2024-1', metadata={'source': './documents/Deep Learning\\\\03-1-backpropagation-handout.pdf', 'page': 0}),\n",
       " Document(page_content='Backpropagation\\n•Backpropagation is an algorithm to compute gradients\\n▶Origins in the 60s in control theory\\n▶Rediscovered many times\\n▶Used for neural networks since the 80s\\n•Given a compute graph, performs\\n1', metadata={'source': './documents/Deep Learning\\\\03-1-backpropagation-handout.pdf', 'page': 1}),\n",
       " Document(page_content='. Forward pass to compute (all) output(s) ( forward propagation )\\n2', metadata={'source': './documents/Deep Learning\\\\03-1-backpropagation-handout.pdf', 'page': 1}),\n",
       " Document(page_content='. Backward pass to compute (all) gradient(s) ( backward propagation )\\n•For us: compute graph typically represents\\n▶Output ˆyof an FNN (given x,θ)\\n▶LossLof an FNN (given (x, y),θ)\\n▶Cost function Jfor an FNN (given {(xi, yi)},θ)\\n•And we are interested in gradients (as we will see)\\n▶W', metadata={'source': './documents/Deep Learning\\\\03-1-backpropagation-handout.pdf', 'page': 1}),\n",
       " Document(page_content='.r.t. weights ( ∇θJ): e.g., for gradient-based training\\n▶W.r.t. intermediate outputs ( ∇zL): e.g., for model debugging\\n▶W.r.t. inputs ( ∇xLof∇xˆy): e.g', metadata={'source': './documents/Deep Learning\\\\03-1-backpropagation-handout.pdf', 'page': 1}),\n",
       " Document(page_content='.r.t. inputs ( ∇xLof∇xˆy): e.g., for sensitivity analysis or\\nadversarial training\\n2 / 17', metadata={'source': './documents/Deep Learning\\\\03-1-backpropagation-handout.pdf', 'page': 1}),\n",
       " Document(page_content='Recap: Gradient\\n•For functions with multiple inputs, there are multiple partial\\nderivatives ; e.g', metadata={'source': './documents/Deep Learning\\\\03-1-backpropagation-handout.pdf', 'page': 2}),\n",
       " Document(page_content='.,\\nf=x2\\n1+ 5x1x2\\n∂\\n∂x1f= 2x1+ 5x2\\n∂\\n∂x2f= 5x1\\n•We can gather them all in a single vector, the gradient off\\n∇x⊤fdef=\\x10\\n∂\\n∂x1J∂\\n∂x2f···∂\\n∂xnf\\x11\\n•For the example above, we obtain\\n∇x⊤f=\\x00\\n2x1+ 5x25x1\\x01\\n∇xf=\\x122x1+ 5x2\\n5x1\\x13\\nNumerator layout (row) Denominator layout (column)\\n3 / 17', metadata={'source': './documents/Deep Learning\\\\03-1-backpropagation-handout.pdf', 'page': 2}),\n",
       " Document(page_content='Compute graphs\\n•Backpropagation generally operates on a compute graph\\n•Directed, acyclic graph that models a computation (as a data\\nflow program )\\n•Vertices correspond to operations\\n•Edges correspond to data passed between operations\\n(typically tensor-valued)\\n•Multiple sources(no incoming edge): inputs, weights, ', metadata={'source': './documents/Deep Learning\\\\03-1-backpropagation-handout.pdf', 'page': 3}),\n",
       " Document(page_content='...\\n•Onesink(no outgoing edge): result\\nFeedforward neural network\\nx z1 z2 W1 W2\\nCorresponding compute graph\\ninput matmulweight1\\nmatmulweight2\\nresultx z1W⊤\\n1 W⊤\\n2\\nz2\\n4 / 17', metadata={'source': './documents/Deep Learning\\\\03-1-backpropagation-handout.pdf', 'page': 3}),\n",
       " Document(page_content='Forward propagation (example)\\n•Compute graph for example output ˆy\\ninput matmulweight1\\nmatmulweight2\\nnorm resultx z1W⊤\\n1 W⊤\\n2\\nz2 ˆy\\n\\x121\\n−2\\x13\\x12−1 1\\n−1−1\\x13 \\x120 4\\n1 0\\x13\\n\\x12−3\\n1\\x13 \\x124\\n−3\\x135\\n•Forward propagation: inputs →result\\n•Edges transport values\\n•For example:\\n1', metadata={'source': './documents/Deep Learning\\\\03-1-backpropagation-handout.pdf', 'page': 4}),\n",
       " Document(page_content='. Provide inputs x,W⊤\\n1,W⊤\\n2\\n2. Evaluate first matmul: z1=W⊤\\n1x\\n3. Evaluate second matmul: z2=W⊤\\n2z1\\n4. Evaluate norm: ˆy=∥z2∥\\n5 / 17', metadata={'source': './documents/Deep Learning\\\\03-1-backpropagation-handout.pdf', 'page': 4}),\n",
       " Document(page_content='Forward propagation\\n•Operators are evaluated in topological order (“forwards”)\\n▶Whenever an operator is evaluated, all its inputs must be available\\n▶Computation is local: only input values are required (the\\nremainder of the compute graph does not matter)\\n•Inputs and/or outputs are generally tensor-valued\\n▶E', metadata={'source': './documents/Deep Learning\\\\03-1-backpropagation-handout.pdf', 'page': 5}),\n",
       " Document(page_content='.g', metadata={'source': './documents/Deep Learning\\\\03-1-backpropagation-handout.pdf', 'page': 5}),\n",
       " Document(page_content='., matmul( A,B) =ABtakes two 2D tensors and produces a\\n2D tensor\\n▶Note: our visual representation of compute graph does not\\nindicate which input is Aand which is B, but the actual compute\\ngraph does (and must do so)\\n•Intermediate results may need to be kept\\n▶To evaluate subsequent operators\\n▶To enable gradient computation with backpropagation\\n•Parallel processing is possible\\n▶Each operator can be evaluated as soon as all its inputs available\\n▶E', metadata={'source': './documents/Deep Learning\\\\03-1-backpropagation-handout.pdf', 'page': 5}),\n",
       " Document(page_content='.g., transformer encoders can operate on all inputs in parallel\\n▶E.g., RNN encoders must process inputs sequentially\\n6 / 17', metadata={'source': './documents/Deep Learning\\\\03-1-backpropagation-handout.pdf', 'page': 5}),\n",
       " Document(page_content='Backward propagation (example)\\n•Backward graph for example output ˆy\\ninput matmulweight1\\nmatmulweight2\\nnorm resultδx δz1δW⊤\\n1δW⊤\\n2\\nδz2δˆy\\n\\x12−2.6\\n−3.8\\x13 \\x12−0.6\\n1.2\\x13\\x12−0.6 1 .2\\n3.2−6.4\\x13 \\x12−2.4 0 .8\\n1.8−0', metadata={'source': './documents/Deep Learning\\\\03-1-backpropagation-handout.pdf', 'page': 6}),\n",
       " Document(page_content='.8\\x13 \\x12−0.6\\n1.2\\x13\\x12−0.6 1 .2\\n3.2−6.4\\x13 \\x12−2.4 0 .8\\n1.8−0.6\\x13\\n\\x120.8\\n−0.6\\x131\\n•Backward propagation: result →gradients\\n•Edges transport gradients\\n▶Consider edge eand define\\nδedef=gradient of result w.r.t', metadata={'source': './documents/Deep Learning\\\\03-1-backpropagation-handout.pdf', 'page': 6}),\n",
       " Document(page_content='.r.t. values on edge e\\nevaluated at the provided inputs\\n•For example:\\n1. Compute all values of forward pass (not shown above)\\n2.δˆy=∇ˆyresult =∇ˆyˆy= 1\\n3.δz2(discussed later)\\n4', metadata={'source': './documents/Deep Learning\\\\03-1-backpropagation-handout.pdf', 'page': 6}),\n",
       " Document(page_content='.δˆy=∇ˆyresult =∇ˆyˆy= 1\\n3.δz2(discussed later)\\n4.δW⊤\\n2andδz1(discussed later)\\n5.δW⊤\\n1andδx(discussed later)\\n7 / 17', metadata={'source': './documents/Deep Learning\\\\03-1-backpropagation-handout.pdf', 'page': 6}),\n",
       " Document(page_content='Backward propagation\\n•δedef=gradient of result w.r.t', metadata={'source': './documents/Deep Learning\\\\03-1-backpropagation-handout.pdf', 'page': 7}),\n",
       " Document(page_content='. values on edge e\\n•Key insight of backpropagation\\n▶Gradients δecan be computed incrementally (akin to forward pass,\\nbut in reverse order)\\n•Operators are evaluated in reverse topological order (“backwards”)\\n▶When operator evaluated, its output gradient(s) must be available\\n▶Computation is local: only input values and output gradient(s) are\\nrequired (the remainder of the compute graph does not matter)\\n▶Recall: intermediate outputs of forward pass required\\n→memory consumption (or recompute)\\n•Gradients are generally tensor-valued\\n▶Convention: same shape as values in forward pass\\n•Intermediate results may need to be kept\\n▶To evaluate gradient for prior operators\\n▶To debug/analyze models\\n•Parallel processing is possible (as before)\\n8 / 17', metadata={'source': './documents/Deep Learning\\\\03-1-backpropagation-handout.pdf', 'page': 7}),\n",
       " Document(page_content='Gradient (single univariate function)\\ninput f resultu y\\n•Output: y=f(u)\\n•Gradient δydef=∇yy= 1\\n•Gradient δudef=∇uy=∂\\n∂uf(u) =f′(u)\\n•Example\\n▶y=σ(u) =σ(0)(logistic function)\\n▶δu=σ′(u) =σ(u)(1−σ(u)) =σ(0)(1−σ(0))\\ninput σ resultu y\\n0 0', metadata={'source': './documents/Deep Learning\\\\03-1-backpropagation-handout.pdf', 'page': 8}),\n",
       " Document(page_content='.5input σ resultδu δy\\n0.25 1\\nForward pass Backward pass\\n9 / 17', metadata={'source': './documents/Deep Learning\\\\03-1-backpropagation-handout.pdf', 'page': 8}),\n",
       " Document(page_content='Gradient (composition of two univariate functions)\\n•Let’s add another operator gin front\\ninput g f resultv u y\\n•Output: y=f(u) =f(g(v))\\n▶Function composition\\n•Gradient: δudef=∇uy=∂\\n∂uf(u) =f′(u) =f′(u)\\n▶Same computation as before (but unow output of g)\\n▶Need to retain uin forward pass to compute f′(u)\\n•Gradient\\nδvdef=∇vy=∂\\n∂vf(g(v))=g′(v)f′(g(v))\\n| {z }\\nchain rule=g′(v)f′(u)\\n=g′(v)δu\\n▶Observe: that’s a local computation at g\\n▶Need: δu→passed backwards from subsequent operators\\n▶Need: v→computed in forward pass\\n▶Need: g′→determined by g\\n10 / 17', metadata={'source': './documents/Deep Learning\\\\03-1-backpropagation-handout.pdf', 'page': 9}),\n",
       " Document(page_content='Example\\nForward pass\\ninput log2 σ resultv u y\\n1 0 0.5\\nBackward pass\\ninput log2 σ resultδv δu δy\\n0.36 0.25 1\\n•δy=∇yy= 1\\n•δu=∇uy=σ′(u)δy=σ(u)(1−σ(u))δy= 0.25·1 = 0 .25\\n•δv=∇vy= log′\\n2(v)δu=1\\nvlog(2)δu≈1', metadata={'source': './documents/Deep Learning\\\\03-1-backpropagation-handout.pdf', 'page': 10}),\n",
       " Document(page_content='.25·1 = 0 .25\\n•δv=∇vy= log′\\n2(v)δu=1\\nvlog(2)δu≈1.44·0.25 = 0 .36\\n11 / 17', metadata={'source': './documents/Deep Learning\\\\03-1-backpropagation-handout.pdf', 'page': 10}),\n",
       " Document(page_content='Gradient (composition of univariate functions)\\n•This generalizes; e.g', metadata={'source': './documents/Deep Learning\\\\03-1-backpropagation-handout.pdf', 'page': 11}),\n",
       " Document(page_content='., consider noperators\\ninput f1 f2 ··· fn resultz0=x z1 z2 zn−1 zn=y\\n•We have\\ny=fn(fn−1(···(f1(x))···))\\n•At each operator fi, the required gradient can be computed as\\nfollows:\\nδzi−1def=∇zi−1y=∂y\\n∂zi−1=∂y\\n∂zi∂zi\\n∂zi−1| {z }\\nchain rule\\n=f′\\ni(zi−1)|{z}\\nlocal derivative· δzi|{z}\\noutput derivative\\n12 / 17', metadata={'source': './documents/Deep Learning\\\\03-1-backpropagation-handout.pdf', 'page': 11}),\n",
       " Document(page_content='Overall gradient\\n•Let’s derive an expression for the gradients individually\\nδzn= 1\\nδzn−1=f′\\nn(zn−1)δzn = f′\\nn(zn−1)\\nδzn−2=f′\\nn−1(zn−2)δzn−1= f′\\nn−1(zn−2)f′\\nn(zn−1)\\nδzn−3=f′\\nn−2(zn−3)δzn−2=f′\\nn−2(zn−3)f′\\nn−1(zn−2)f′\\nn(zn−1)\\n', metadata={'source': './documents/Deep Learning\\\\03-1-backpropagation-handout.pdf', 'page': 12}),\n",
       " Document(page_content='...\\n•Gradient is product of local gradients along the path from\\nthe result to the resp. edge\\n•Backpropagation avoids repeated computations by\\n1. Proceeding backwards\\n2. Using the chain rule\\n13 / 17', metadata={'source': './documents/Deep Learning\\\\03-1-backpropagation-handout.pdf', 'page': 12}),\n",
       " Document(page_content='Gradient (multiple inputs)\\n•Operators often have multiple inputs; e.g', metadata={'source': './documents/Deep Learning\\\\03-1-backpropagation-handout.pdf', 'page': 13}),\n",
       " Document(page_content='.g., a simple linear unit\\ninput linear unitweight\\nbiasresultxw\\nby\\n•In the forward pass, the operator computes\\ny=f(x, w, b ) =wx+b\\n•In the backward pass, we compute gradients of result w.r.t', metadata={'source': './documents/Deep Learning\\\\03-1-backpropagation-handout.pdf', 'page': 13}),\n",
       " Document(page_content='. each\\nedge as before (using the chain rule)\\nδy= 1\\nδx=∇xy=∇xf(w, x, b )·δy=w·1\\nδw=∇wy=∇wf(w, x, b )·δy=x·1\\nδb=∇by=∇bf(w, x, b )·δy= 1·1\\n•Consider each input separately and reuse incoming δ-value\\n14 / 17', metadata={'source': './documents/Deep Learning\\\\03-1-backpropagation-handout.pdf', 'page': 13}),\n",
       " Document(page_content='Gradient (multiple outputs)\\n•Operators may have multiple outputs; e.g., consider\\n▶E.g., operator f(x)may output nvalues, say, z1=f1(x), ...,\\nzn=fn(x)\\n▶During backpropagation, we obtain δz1, ..', metadata={'source': './documents/Deep Learning\\\\03-1-backpropagation-handout.pdf', 'page': 14}),\n",
       " Document(page_content='..., δzn▶We are interested in\\nδx=∇xy=∂y\\n∂x=nX\\nk=1∂y\\n∂zk∂zk\\n∂x\\n| {z }\\nmultivariate chain rule\\n=nX\\nk=1f′\\nk(x)δzk\\n▶Consider each output independently and sum up\\n15 / 17', metadata={'source': './documents/Deep Learning\\\\03-1-backpropagation-handout.pdf', 'page': 14}),\n",
       " Document(page_content='Gradient (multiple uses)\\n•Sometimes an operator’s output is “used” multiple times\\n▶E.g., the output of an operator g(x)is used ntimes\\n▶That’s equivalent to a single operator fwithnidentical outputs\\n(i', metadata={'source': './documents/Deep Learning\\\\03-1-backpropagation-handout.pdf', 'page': 15}),\n",
       " Document(page_content='.e', metadata={'source': './documents/Deep Learning\\\\03-1-backpropagation-handout.pdf', 'page': 15}),\n",
       " Document(page_content='., zk=fk(x) =g(x)), each being used once\\n▶Using the results from the previous slide with fdefined in this way:\\nδx=∇xy=nX\\nk=1f′\\nk(x)δzk=nX\\nk=1g′(x)δzk\\n=g′(x)nX\\nk=1δk\\n▶Sum up all incoming δ-values and proceed as before\\n16 / 17', metadata={'source': './documents/Deep Learning\\\\03-1-backpropagation-handout.pdf', 'page': 15}),\n",
       " Document(page_content='Gradient computation in general\\n•Consider an operator f:RI→RO\\n•Forward pass: vout=f(vin)withvin∈RIandvout∈RO\\nfvinvout\\n•Backward pass: δin=Jf(vin)⊤δoutwithδout∈ROandδin∈RI\\nJfδinδout\\nwhere we use the Jacobian Jf∈RO×Igiven by\\nJf=∇v⊤\\ninf=\\uf8eb\\n\\uf8ec\\uf8ed∇v⊤\\ninvout\\n1\\n', metadata={'source': './documents/Deep Learning\\\\03-1-backpropagation-handout.pdf', 'page': 16}),\n",
       " Document(page_content='...\\n∇v⊤\\ninvout\\nO\\uf8f6\\n\\uf8f7\\uf8f8=\\uf8eb\\n\\uf8ec\\uf8ec\\uf8ed∂\\n∂vin\\n1vout\\n1. . .∂\\n∂vin\\nIvout\\n1\\n.........\\n∂\\n∂vin\\n1vout\\nO. .', metadata={'source': './documents/Deep Learning\\\\03-1-backpropagation-handout.pdf', 'page': 16}),\n",
       " Document(page_content='.∂\\n∂vin\\nIvout\\nO\\uf8f6\\n\\uf8f7\\uf8f7\\uf8f8\\n•Intuitively, f′(vin)δoutnow becomes Jf(vin)⊤δout\\n▶Can be derived by “rewriting” the discussions on multiple\\ninputs/outputs from the previous slides into matrix form\\n•More in exercises and tutorials17 / 17', metadata={'source': './documents/Deep Learning\\\\03-1-backpropagation-handout.pdf', 'page': 16})]"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "docs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the Retrieval QA Chain to integrate the database and LLM\n",
    "# from langchain.chains import RetrievalQA\n",
    "from langchain_openai import OpenAI\n",
    "\n",
    "qa = RetrievalQA.from_chain_type(\n",
    "    OpenAI(model_name=\"gpt-3.5-turbo-instruct\", temperature=0, openai_api_key=openai_api_key), chain_type=\"stuff\", retriever=docstorage.as_retriever())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "query = (\"\"\"A helpful planner providing a step by step intermediate learning goal in order to achieve provided ultimate learning goal. User will provide the end goal that they want to learn, and you as a helper have to give them the best pathway to reach the goal. In each intermediate steps, sub-steps can be provided as well. If they ask more about certain sub-step, give step by step guide to that intermediate step as you gave general guide for the learning goal. Return the result in JSON file, the structure of JSON file is following: \n",
    " \"id\": \"1\",\n",
    "    \"title\": \"\",\n",
    "    \"children\": [\n",
    "      {.\n",
    "        \"id\": \"1.1\",\n",
    "        \"title\": \"\",\n",
    "        \"description\": \"\",\n",
    "        \"links\": [\"\"]\n",
    "      },\n",
    "where 'id' refers to the depth of the step, 'title' shows the topic of learning, 'children' includes ids of sub steps to the current step, 'description' includes the learning goal of the current step, 'link' for reference url links for learning (links for academic paper, articles).\"\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run the chain on the query provided\n",
    "output = (qa.run(query))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "{\n",
      "  \"id\": \"1\",\n",
      "  \"title\": \"Learning Long-Range Dependencies in Sequence Transduction Tasks\",\n",
      "  \"children\": [\n",
      "    {\n",
      "      \"id\": \"1.1\",\n",
      "      \"title\": \"Understanding the concept of long-range dependencies\",\n",
      "      \"description\": \"Learn about the challenges of learning long-range dependencies in sequence transduction tasks and why it is important.\",\n",
      "      \"links\": [\"https://arxiv.org/abs/1708.02182\"]\n",
      "    },\n",
      "    {\n",
      "      \"id\": \"1.2\",\n",
      "      \"title\": \"Increasing learning rate linearly for the first warmup steps\",\n",
      "      \"description\": \"Learn about the concept of warmup steps and how increasing the learning rate linearly can help in learning long-range dependencies.\",\n",
      "      \"links\": [\"https://arxiv.org/abs/1706.03762\"]\n",
      "    },\n",
      "    {\n",
      "      \"id\": \"1.3\",\n",
      "      \"title\": \"Decreasing learning rate proportionally to the inverse square root of the step number\",\n",
      "      \"description\": \"Understand how decreasing the learning rate proportionally to the inverse square root of the step number can help in learning long-range dependencies.\",\n",
      "      \"links\": [\"https://arxiv.org/abs/1706\n"
     ]
    }
   ],
   "source": [
    "print(output)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
